{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uses the Tensoflow machine learning library to train the Poisson \n",
    "autoencoder inverting network (PAIN) to reconstruct compressed \n",
    "MNIST images with Poisson noise, and saves the trained model.\n",
    "\n",
    "Authors: Fabian Santiago\n",
    "Last Update: August 18, 2024\n",
    "'''\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from skimage.util.shape import view_as_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for TensorFlow and Keras\n",
    "tf.random.set_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sliding window compression\n",
    "def median_downsampling(in_image, cmp_dim):\n",
    "    if cmp_dim < 15:\n",
    "        # Compression dimension to window size\n",
    "        window_size = 28//cmp_dim\n",
    "        # Create sliding windows\n",
    "        windows = view_as_windows(in_image, (window_size, window_size), step=window_size)\n",
    "        # Calculate the median over each window\n",
    "        return np.median(windows, axis=(2, 3))\n",
    "    else:\n",
    "        return in_image\n",
    "\n",
    "# Define compression of entries in an array\n",
    "def down_sample_list(in_array,cmp_dim):\n",
    "    out_array = np.empty((len(in_array), cmp_dim, cmp_dim))\n",
    "    \n",
    "    # Iterate over array\n",
    "    for idx, image in enumerate(in_array):\n",
    "        out_array[idx] = median_downsampling(image,cmp_dim)\n",
    "        \n",
    "    # Return array containing compressed entries \n",
    "    return out_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Dataset\n",
    "########################\n",
    "# Load only MNIST images\n",
    "(clean_train, _), (clean_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data to [0,1]\n",
    "# clean_train, clean_test = clean_train/255, clean_test/255\n",
    "\n",
    "# Dimension of the compressed/noisy images (width=height)  \n",
    "# cmp_dim = 4 (for 4x4), 7, 14, or 28\n",
    "cmp_dim = 7\n",
    "\n",
    "\n",
    "# Step 1: Compress images using median in sliding window\n",
    "cmp_train = down_sample_list(clean_train, cmp_dim)\n",
    "cmp_test  = down_sample_list(clean_test, cmp_dim)\n",
    "\n",
    "# Step 2: Add Poisson noise to compressed images\n",
    "noisy_train = np.clip(np.random.poisson(lam=cmp_train),0,255)\n",
    "noisy_test  = np.clip(np.random.poisson(lam=cmp_test),0,255)\n",
    "\n",
    "# Step 3: Reshape Input Arrays\n",
    "noisy_train = np.array([matrix.reshape(cmp_dim**2,) for matrix in noisy_train/255])\n",
    "noisy_test  = np.array([matrix.reshape(cmp_dim**2,) for matrix in noisy_test/255])\n",
    "\n",
    "# Step 4: Reshape Expected Ouput Arraus\n",
    "clean_train = np.array([matrix.reshape(28**2,) for matrix in clean_train/255])\n",
    "clean_test  = np.array([matrix.reshape(28**2,) for matrix in clean_test/255])\n",
    "\n",
    "# noisy_train = \n",
    "\n",
    "# # 'clean_train' -> Original 28x28 MNIST images\n",
    "# # 'noisy_train' -> Compressed/Noisy MNIST images (cmp_dim x cmp_dim)\n",
    "# MNIST_data  = 'MNIST_data/'+'MNIST_'+str(cmp_dim)+'x'+str(cmp_dim)+'_Training.npz'\n",
    "# DATA        = np.load(MNIST_data)\n",
    "# clean_train = DATA['clean_train'][0:50000]\n",
    "# noisy_train = DATA['noisy_train'][0:50000]\n",
    "# clean_test  = DATA['clean_train'][50000:]\n",
    "# noisy_test  = DATA['noisy_train'][50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data Values\n",
    "in_dim   = cmp_dim  # This is the input dimemsions in_dim x in_dim\n",
    "in_size  = int(in_dim*in_dim)\n",
    "out_dim  = 28 # never changes because MNIST original are 28 x 28\n",
    "out_size = int(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Poisson autoencoder inverting network (PAIN)\n",
    "####################################################\n",
    "\n",
    "encoder_size = 256 # Size of intermediate layer\n",
    "\n",
    "# Define model architecture\n",
    "def MODEL(input_shape):\n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=(input_shape,))\n",
    "\n",
    "    # First Decoder\n",
    "    Dec1 = tf.keras.layers.Dense(out_size, activation='sigmoid')(inputs)\n",
    "\n",
    "    # Encoder\n",
    "    Enc2_hidden = tf.keras.layers.Dense(encoder_size, activation='sigmoid')(Dec1)\n",
    "    Enc2 = tf.keras.layers.Dense(in_size, activation='sigmoid')(Enc2_hidden)\n",
    "\n",
    "    # Second Decoder\n",
    "    Dec2_hidden = tf.keras.layers.Dense(encoder_size, activation='sigmoid')(Enc2)\n",
    "    outputs = tf.keras.layers.Dense(out_size, activation='sigmoid')(Dec2_hidden)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "PAIN = MODEL(in_size)\n",
    "\n",
    "# Create an RMSProp optimizer with a specific learning rate\n",
    "RMSp = tf.keras.optimizers.RMSprop(learning_rate=0.05)\n",
    "\n",
    "# Compile the model\n",
    "###################\n",
    "PAIN.compile(optimizer=RMSp, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PAIN.fit(noisy_train, clean_train, epochs=600, batch_size=250,validation_data=(noisy_test, clean_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Trained Model\n",
    "####################\n",
    "\n",
    "# Model directory \n",
    "directory = \"Trained_Models\"\n",
    "\n",
    "# If directory does not exist, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Save trained model\n",
    "PAIN.save(f'Trained_Models/PAIN{cmp_dim}x{cmp_dim}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply PAIN to all training inputs\n",
    "pred_train_out = PAIN.predict(noisy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples: training set\n",
    "########################################################\n",
    "\n",
    "# Create a 3 by 4 subplot handle\n",
    "fig, axes = plt.subplots(3, 4, figsize=(11, 8))\n",
    "axes      = axes.flatten()\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Application of PAIN Architecture\\n(Training Set)',fontsize=20,fontweight='bold', fontfamily='serif')\n",
    "\n",
    "# Shift through training dataset \n",
    "sft_idx = 90\n",
    "\n",
    "# Plot noisy and decompressed realizations in each subplot\n",
    "for idx in range(4):\n",
    "    # Plot noisy\n",
    "    axes[idx].imshow(noisy_train[idx+sft_idx].reshape(cmp_dim,cmp_dim),cmap='gray')\n",
    "    axes[idx].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx].set_yticks([]) # Remove yticklabels\n",
    "    axes[idx].set_xlabel('â‡©',fontdict={'fontsize': 25, 'fontweight': 'bold', 'fontfamily': 'serif'})\n",
    "    # Plot decompressed with PAIN\n",
    "    axes[idx+4].imshow(pred_train_out[idx+sft_idx].reshape(28,28),cmap='gray')\n",
    "    axes[idx+4].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx+4].set_yticks([]) # Remove yticklabels\n",
    "    \n",
    "    # Plot original\n",
    "    axes[idx+8].imshow(clean_train[idx+sft_idx].reshape(28,28),cmap='gray')\n",
    "    axes[idx+8].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx+8].set_yticks([]) # Remove yticklabels\n",
    "    \n",
    "# Set ylables \n",
    "axes[0].set_ylabel(f'{cmp_dim} x {cmp_dim}\\n(Input)\\nCompressed/Noisy',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "axes[4].set_ylabel('28 x 28\\n(Output)\\nPAIN Reconstruction',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "axes[8].set_ylabel('28 x 28\\n(Original)\\nMNIST',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "\n",
    "# Adjust layout to decrease padding between subplots\n",
    "plt.subplots_adjust(wspace=-0.2, hspace=0.25)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PAIN to all validation inputs\n",
    "pred_test_out = PAIN.predict(noisy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples: validation set \n",
    "################################\n",
    "\n",
    "# Create a 3 by 4 subplot handle\n",
    "fig, axes = plt.subplots(3, 4, figsize=(11, 8))\n",
    "axes      = axes.flatten()\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Application of PAIN Architecture\\n(Validation Set)',fontsize=20,fontweight='bold', fontfamily='serif')\n",
    "\n",
    "# Shift through validation dataset \n",
    "sft_idx   = 10\n",
    "\n",
    "# Plot noisy and decompressed realizations in each subplot\n",
    "for idx in range(4):\n",
    "    # Plot noisy\n",
    "    axes[idx].imshow(noisy_test[idx+sft_idx].reshape(cmp_dim,cmp_dim),cmap='gray')\n",
    "    axes[idx].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx].set_yticks([]) # Remove yticklabels\n",
    "    axes[idx].set_xlabel('â‡©',fontdict={'fontsize': 25, 'fontweight': 'bold', 'fontfamily': 'serif'})\n",
    "    # Plot decompressed with PAIN\n",
    "    axes[idx+4].imshow(pred_test_out[idx+sft_idx].reshape(28,28),cmap='gray')\n",
    "    axes[idx+4].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx+4].set_yticks([]) # Remove yticklabels\n",
    "    \n",
    "    # Plot original\n",
    "    axes[idx+8].imshow(clean_test[idx+sft_idx].reshape(28,28),cmap='gray')\n",
    "    axes[idx+8].set_xticks([]) # Remove xticklabels\n",
    "    axes[idx+8].set_yticks([]) # Remove yticklabels\n",
    "    \n",
    "# Set ylables \n",
    "axes[0].set_ylabel(f'{cmp_dim} x {cmp_dim}\\n(Input)\\nCompressed/Noisy',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "axes[4].set_ylabel('28 x 28\\n(Output)\\nPAIN Reconstruction',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "axes[8].set_ylabel('28 x 28\\n(Original)\\nMNIST',fontdict={'fontsize': 12, 'fontfamily': 'serif'})\n",
    "\n",
    "# Adjust layout to decrease padding between subplots\n",
    "plt.subplots_adjust(wspace=-0.2, hspace=0.25)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
